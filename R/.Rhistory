a_est_for_country <- rbind(a_est_for_country,
data.frame(x=mean(eta_samples[time_it,]),y=mean(eta_samples[time_it+1,])))
}
A_estimation_matrix <- rbind(A_estimation_matrix, a_est_for_country)
##Estimate sigma
m2 <- data.frame(x=c(),y=c())
for(time_it in 2:(length(ALL_TIMES))){
m2 <- rbind(m2, data.frame(x=mean(eta_samples[time_it,]),y=old_A*mean(eta_samples[time_it-1,])))
}
old_sigma[[country_var]] <- country_sigma
sigma[[country_var]] <- sqrt(sum((m2$x-m2$y)^2)/nrow(m2))
print(paste("Country:", country_var,
"Type: ", type_var,
"Old sigma:", old_sigma[[country_var]],
"New sigma:", sigma[[country_var]],
"diff:", abs(old_sigma[[country_var]]-sigma[[country_var]]) ))
##save the samples
esl <- list(x=eta_samples)
names(esl) <- paste(type_var,country_var,category_var,sep="_")
per_cat_eta_samples_list <- c(per_cat_eta_samples_list,esl)
results[country==country_var & category==category_var & type ==type_var, "sigma"] <- sigma[[country_var]]
} ##end for country
##UPDATE A
a_lm <- lm(y~x-1,A_estimation_matrix)
old_A <- A
A <- as.numeric(a_lm$coefficients[1])
print(paste("Old A:", old_A, " New A:", A, "diff:", abs(old_A-A) ))
results[category==category_var & type == type_var]$a <- A
} ##end while iter
eta_samples_test <- c(eta_samples_test,per_cat_eta_samples_list)
} ## category for loop
}
full_data
lapply(eta_samples_test,function(l){apply(l,1,mean)})
get_resampled <- function(t, eta_sample_set, sigma, data,
times=ALL_TIMES,
n_samples=N_SAMPLES_PER_TIME_PERIOD){
##get likelihoods
date_data <- data[data$datetime == times[t]]
weights <- rep(NA,n_samples)
for(sample_num in 1:n_samples){
p_param <- 1/(1+ exp(-(date_data$mu_est+date_data$v_est+eta_sample_set[sample_num])))
weights[sample_num] <- dbinom(date_data$c_wrt,date_data$s_wrt, p_param)
}
weights <- weights/sum(weights)
#print(weights[weights > .001])
##it seems wise to resample here:
sampled <- sample(eta_sample_set,size=n_samples,replace=T,prob=weights)
#print(paste("unique samples: ", length(unique(sampled))/n_samples))
return(sampled)
}
interesting_categories <- c("war")#,"human_rights",
results <- data.table(expand.grid(country=ALL_COUNTRIES,
category=interesting_categories,
type=c("TWITTER"),
a=-99,sigma=-99,stringsAsFactors=F))
eta_samples_test <- list()
##for each word
for(type_var in c("TWITTER")){
for(category_var in interesting_categories){
##A is global across the word
##sigma is defined per country
A <- 1.5
old_A <- -1
sigma <- as.list(sapply(ALL_COUNTRIES,function(l){return(2)}))
old_sigma <- as.list(sapply(ALL_COUNTRIES,function(l){return(-1)}))
turn_iter <- 0
A_estimation_matrix <- data.frame(x=c(),y=c())
##while not convergence or max iterations
while( (any(abs(unlist(old_sigma)-unlist(sigma)) > THRESHOLD) | abs(A-old_A) > THRESHOLD) &
turn_iter < MAX_ITERATION ) {
per_cat_eta_samples_list <- list()
turn_iter <- turn_iter + 1
print(paste0("turn: ",turn_iter))
##get samples for each country
for(country_var in ALL_COUNTRIES){
print(paste("STARTING:::::::: ",country_var,category_var))
data <- full_data[country == country_var & category == category_var & type == type_var]
country_sigma <- sigma[[country_var]]
##E STEP
##initialize with p0
eta_samples <- matrix(-1,nrow=length(ALL_TIMES),ncol=N_SAMPLES_PER_TIME_PERIOD)
init_date_data <- data[data$datetime == ALL_TIMES[1]]
init_p <- init_date_data$c_wrt/init_date_data$s_wrt
mean_param <- log(init_p/(1-init_p)) - init_date_data$v_est - init_date_data$mu_est
print(mean_param)
eta_samples[1,] <- rnorm(N_SAMPLES_PER_TIME_PERIOD,mean_param,country_sigma)
##Forward Filtering (SIR)
for(t in 2:N_TIME_PERIODS){
#print(paste0("\t",t))
eta_samples[t,] <- rnorm(n=N_SAMPLES_PER_TIME_PERIOD,
mean=A*eta_samples[t-1,],
sd=rep(country_sigma,N_SAMPLES_PER_TIME_PERIOD))
eta_samples[t,] <- get_resampled(t, eta_samples[t,],country_sigma,data)
}
##Backwards Smoothing
for(t in seq(N_TIME_PERIODS,1,-1)){
eta_samples[t, ] <- sample(eta_samples[t,],size=N_SAMPLES_PER_TIME_PERIOD,replace=T)
}
##prep for M step of A, M step for sigma
##Add the samples from here into the eventual re-estimation of
a_est_for_country <- data.frame(x=c(),y=c())
for(time_it in 1:(length(ALL_TIMES)-1)){
a_est_for_country <- rbind(a_est_for_country,
data.frame(x=mean(eta_samples[time_it,]),y=mean(eta_samples[time_it+1,])))
}
A_estimation_matrix <- rbind(A_estimation_matrix, a_est_for_country)
##Estimate sigma
m2 <- data.frame(x=c(),y=c())
for(time_it in 2:(length(ALL_TIMES))){
m2 <- rbind(m2, data.frame(x=mean(eta_samples[time_it,]),y=old_A*mean(eta_samples[time_it-1,])))
}
old_sigma[[country_var]] <- country_sigma
sigma[[country_var]] <- sqrt(sum((m2$x-m2$y)^2)/nrow(m2))
print(paste("Country:", country_var,
"Type: ", type_var,
"Old sigma:", old_sigma[[country_var]],
"New sigma:", sigma[[country_var]],
"diff:", abs(old_sigma[[country_var]]-sigma[[country_var]]) ))
##save the samples
esl <- list(x=eta_samples)
names(esl) <- paste(type_var,country_var,category_var,sep="_")
per_cat_eta_samples_list <- c(per_cat_eta_samples_list,esl)
results[country==country_var & category==category_var & type ==type_var, "sigma"] <- sigma[[country_var]]
} ##end for country
##UPDATE A
a_lm <- lm(y~x-1,A_estimation_matrix)
old_A <- A
A <- as.numeric(a_lm$coefficients[1])
print(paste("Old A:", old_A, " New A:", A, "diff:", abs(old_A-A) ))
results[category==category_var & type == type_var]$a <- A
} ##end while iter
eta_samples_test <- c(eta_samples_test,per_cat_eta_samples_list)
} ## category for loop
}
country <- "algeria"
for(i in 2:nrow(full_data)){
if(full_data[i,]$country == country){
full_data[i,]$c_wrt <- .8*full_data[i-1,]$c_wrt[1]
} else {
country <- full_data[i,]$country
}
}
full_dataa
full_data
country <- "algeria"
for(i in 2:nrow(full_data)){
if(full_data[i,]$country == country){
full_data[i,]$c_wrt <- .9*full_data[i-1,]$c_wrt[1]
} else {
country <- full_data[i,]$country
}
}
full_data
interesting_categories <- c("war")#,"human_rights",
results <- data.table(expand.grid(country=ALL_COUNTRIES,
category=interesting_categories,
type=c("TWITTER"),
a=-99,sigma=-99,stringsAsFactors=F))
eta_samples_test <- list()
##for each word
for(type_var in c("TWITTER")){
for(category_var in interesting_categories){
##A is global across the word
##sigma is defined per country
A <- 1.5
old_A <- -1
sigma <- as.list(sapply(ALL_COUNTRIES,function(l){return(2)}))
old_sigma <- as.list(sapply(ALL_COUNTRIES,function(l){return(-1)}))
turn_iter <- 0
A_estimation_matrix <- data.frame(x=c(),y=c())
##while not convergence or max iterations
while( (any(abs(unlist(old_sigma)-unlist(sigma)) > THRESHOLD) | abs(A-old_A) > THRESHOLD) &
turn_iter < MAX_ITERATION ) {
per_cat_eta_samples_list <- list()
turn_iter <- turn_iter + 1
print(paste0("turn: ",turn_iter))
##get samples for each country
for(country_var in ALL_COUNTRIES){
print(paste("STARTING:::::::: ",country_var,category_var))
data <- full_data[country == country_var & category == category_var & type == type_var]
country_sigma <- sigma[[country_var]]
##E STEP
##initialize with p0
eta_samples <- matrix(-1,nrow=length(ALL_TIMES),ncol=N_SAMPLES_PER_TIME_PERIOD)
init_date_data <- data[data$datetime == ALL_TIMES[1]]
init_p <- init_date_data$c_wrt/init_date_data$s_wrt
mean_param <- log(init_p/(1-init_p)) - init_date_data$v_est - init_date_data$mu_est
print(mean_param)
eta_samples[1,] <- rnorm(N_SAMPLES_PER_TIME_PERIOD,mean_param,country_sigma)
##Forward Filtering (SIR)
for(t in 2:N_TIME_PERIODS){
#print(paste0("\t",t))
eta_samples[t,] <- rnorm(n=N_SAMPLES_PER_TIME_PERIOD,
mean=A*eta_samples[t-1,],
sd=rep(country_sigma,N_SAMPLES_PER_TIME_PERIOD))
eta_samples[t,] <- get_resampled(t, eta_samples[t,],country_sigma,data)
}
##Backwards Smoothing
for(t in seq(N_TIME_PERIODS,1,-1)){
eta_samples[t, ] <- sample(eta_samples[t,],size=N_SAMPLES_PER_TIME_PERIOD,replace=T)
}
##prep for M step of A, M step for sigma
##Add the samples from here into the eventual re-estimation of
a_est_for_country <- data.frame(x=c(),y=c())
for(time_it in 1:(length(ALL_TIMES)-1)){
a_est_for_country <- rbind(a_est_for_country,
data.frame(x=mean(eta_samples[time_it,]),y=mean(eta_samples[time_it+1,])))
}
A_estimation_matrix <- rbind(A_estimation_matrix, a_est_for_country)
##Estimate sigma
m2 <- data.frame(x=c(),y=c())
for(time_it in 2:(length(ALL_TIMES))){
m2 <- rbind(m2, data.frame(x=mean(eta_samples[time_it,]),y=old_A*mean(eta_samples[time_it-1,])))
}
old_sigma[[country_var]] <- country_sigma
sigma[[country_var]] <- sqrt(sum((m2$x-m2$y)^2)/nrow(m2))
print(paste("Country:", country_var,
"Type: ", type_var,
"Old sigma:", old_sigma[[country_var]],
"New sigma:", sigma[[country_var]],
"diff:", abs(old_sigma[[country_var]]-sigma[[country_var]]) ))
##save the samples
esl <- list(x=eta_samples)
names(esl) <- paste(type_var,country_var,category_var,sep="_")
per_cat_eta_samples_list <- c(per_cat_eta_samples_list,esl)
results[country==country_var & category==category_var & type ==type_var, "sigma"] <- sigma[[country_var]]
} ##end for country
##UPDATE A
a_lm <- lm(y~x-1,A_estimation_matrix)
old_A <- A
A <- as.numeric(a_lm$coefficients[1])
print(paste("Old A:", old_A, " New A:", A, "diff:", abs(old_A-A) ))
results[category==category_var & type == type_var]$a <- A
} ##end while iter
eta_samples_test <- c(eta_samples_test,per_cat_eta_samples_list)
} ## category for loop
}
A_estimation_matrix
interesting_categories <- c("war")#,"human_rights",
results <- data.table(expand.grid(country=ALL_COUNTRIES,
category=interesting_categories,
type=c("TWITTER"),
a=-99,sigma=-99,stringsAsFactors=F))
eta_samples_test <- list()
##for each word
for(type_var in c("TWITTER")){
for(category_var in interesting_categories){
##A is global across the word
##sigma is defined per country
A <- 1.5
old_A <- -1
sigma <- as.list(sapply(ALL_COUNTRIES,function(l){return(2)}))
old_sigma <- as.list(sapply(ALL_COUNTRIES,function(l){return(-1)}))
turn_iter <- 0
A_estimation_matrix <- data.frame(x=c(),y=c())
##while not convergence or max iterations
while( (any(abs(unlist(old_sigma)-unlist(sigma)) > THRESHOLD) | abs(A-old_A) > THRESHOLD) &
turn_iter < MAX_ITERATION ) {
per_cat_eta_samples_list <- list()
turn_iter <- turn_iter + 1
print(paste0("turn: ",turn_iter))
##get samples for each country
for(country_var in ALL_COUNTRIES){
print(paste("STARTING:::::::: ",country_var,category_var))
data <- full_data[country == country_var & category == category_var & type == type_var]
country_sigma <- sigma[[country_var]]
##E STEP
##initialize with p0
eta_samples <- matrix(-1,nrow=length(ALL_TIMES),ncol=N_SAMPLES_PER_TIME_PERIOD)
init_date_data <- data[data$datetime == ALL_TIMES[1]]
init_p <- init_date_data$c_wrt/init_date_data$s_wrt
mean_param <- log(init_p/(1-init_p)) - init_date_data$v_est - init_date_data$mu_est
print(mean_param)
eta_samples[1,] <- rnorm(N_SAMPLES_PER_TIME_PERIOD,mean_param,country_sigma)
##Forward Filtering (SIR)
for(t in 2:N_TIME_PERIODS){
#print(paste0("\t",t))
eta_samples[t,] <- rnorm(n=N_SAMPLES_PER_TIME_PERIOD,
mean=A*eta_samples[t-1,],
sd=rep(country_sigma,N_SAMPLES_PER_TIME_PERIOD))
eta_samples[t,] <- get_resampled(t, eta_samples[t,],country_sigma,data)
}
##Backwards Smoothing
for(t in seq(N_TIME_PERIODS,1,-1)){
eta_samples[t, ] <- sample(eta_samples[t,],size=N_SAMPLES_PER_TIME_PERIOD,replace=T)
}
##prep for M step of A, M step for sigma
##Add the samples from here into the eventual re-estimation of
a_est_for_country <- data.frame(x=c(),y=c())
for(time_it in 1:(length(ALL_TIMES)-1)){
a_est_for_country <- rbind(a_est_for_country,
data.frame(x=mean(eta_samples[time_it,]),y=mean(eta_samples[time_it+1,])))
}
A_estimation_matrix <- rbind(A_estimation_matrix, a_est_for_country)
##Estimate sigma
m2 <- data.frame(x=c(),y=c())
for(time_it in 2:(length(ALL_TIMES))){
m2 <- rbind(m2, data.frame(x=mean(eta_samples[time_it,]),y=old_A*mean(eta_samples[time_it-1,])))
}
old_sigma[[country_var]] <- country_sigma
sigma[[country_var]] <- sqrt(sum((m2$x-m2$y)^2)/nrow(m2))
print(paste("Country:", country_var,
"Type: ", type_var,
"Old sigma:", old_sigma[[country_var]],
"New sigma:", sigma[[country_var]],
"diff:", abs(old_sigma[[country_var]]-sigma[[country_var]]) ))
##save the samples
esl <- list(x=eta_samples)
names(esl) <- paste(type_var,country_var,category_var,sep="_")
per_cat_eta_samples_list <- c(per_cat_eta_samples_list,esl)
results[country==country_var & category==category_var & type ==type_var, "sigma"] <- sigma[[country_var]]
} ##end for country
##UPDATE A
a_lm <- lm(y~x-1,A_estimation_matrix)
old_A <- A
A <- as.numeric(a_lm$coefficients[1])
print(paste("Old A:", old_A, " New A:", A, "diff:", abs(old_A-A) ))
results[category==category_var & type == type_var]$a <- A
} ##end while iter
eta_samples_test <- c(eta_samples_test,per_cat_eta_samples_list)
} ## category for loop
}
lapply(eta_samples_test,function(l){apply(l,1,mean)})
DATA_DIR <- "raw_data"
library(lubridate)
library(dplyr)
library(tidyr)
library(data.table)
library(ggplot2)
library(bit64)
library(stringr)
remove_countries <- c("sudan","qatar")
TERMS_TO_REMOVE <- c("im","saudi","pkk","dr")
inv_logis <- function(x){return(log(x/(1-x)))}
concept_to_category <- fread(file.path(DATA_DIR,"goldstone_to_category.tsv"),header=F)
setnames(concept_to_category,c("concept","category"))
concept_to_category$concept <- tolower(concept_to_category$concept)
ctm <- fread(file.path(DATA_DIR,"country_to_articles.csv"))
news_df <- fread(file.path(DATA_DIR,"news_term_counts.tsv"))
setnames(news_df, c("date","articleNumber","concept","raw"))
news_df <- merge(news_df, ctm, by=c("date","articleNumber"),allow.cartesian=T)
##ck bad; looks okay
news_df <- news_df[! (raw %in% TERMS_TO_REMOVE)]
ck_bad_raw <- news_df[,length(unique(paste0(date,articleNumber))),by=c("raw","concept")]%>% arrange(-V1)
##cleaning more
news_df$datetime <- ymd(news_df$date)
news_df$concept <- tolower(news_df$concept)
news_df <- merge(news_df,concept_to_category,by="concept",all.x=T)
news_df <- news_df[!is.na(category)] #things we clean post-hoc from concept -> category list
news_df$unique_id <- with(news_df, paste0(articleNumber,date))
ck_lame_cat <- news_df[,length(unique(unique_id)),by=c("category")] %>% arrange(-V1)
##Lets toss the low-lying categories - < 10000
news_df <- news_df[category %in% ck_lame_cat[V1 > 10000,]$category]
news_df$country[news_df$country =="saudi arabia"] <- "saudi_arabia"
news_df$country[news_df$country =="united arab emirates"] <- "uae"
news_df$md <- paste(year(news_df$datetime),month(news_df$datetime),sep="-")
##number of users per category per country per month - in other words, c_r,w,t
articles_per_cat <- news_df[,length(unique(unique_id)),by=c("category","country","md")]
setnames(articles_per_cat,"V1","c_wrt")
ctm$md <- paste(year(ctm$date),month(ctm$date),sep="-")
articles_per_month <- ctm[,length(articleNumber),by=c("md","country")]
setnames(articles_per_day,c(""))
articles_per_month$country[articles_per_month$country =="saudi arabia"] <- "saudi_arabia"
articles_per_month$country[articles_per_month$country =="united arab emirates"] <- "uae"
setnames(articles_per_month,"V1","s_wrt")
###Final clean and merge
full_news_data <- merge(articles_per_cat,articles_per_month,by=c("country","md"))
full_news_data$datetime <- ymd(paste(full_news_data$md,"-01",sep=""))
full_news_data$md <- NULL
##only use counts from the full dataset
ALL_NEWS_CATEGORIES <- unique(full_news_data$category)
ALL_NEWS_COUNTRIES <- unique(full_news_data$country)
ALL_NEWS_TIMES <- unique(full_news_data$datetime)
full_factorial_data <- data.table(expand.grid(country=ALL_NEWS_COUNTRIES,
datetime=ALL_NEWS_TIMES,
category=ALL_NEWS_CATEGORIES,
stringsAsFactors=F))
full_factorial_data <- merge(full_factorial_data,full_news_data,by=c("country","datetime","category"),all.x=T)
full_factorial_data[is.na(c_wrt)]$c_wrt <- 0
full_factorial_data[is.na(s_wrt)]$s_wrt <- 0
full_news_data <- full_factorial_data
ctm$ymd_date <- ymd(paste(ctm$md,"-01",sep=""))
news_df$ymd_date <- ymd(paste(news_df$md,"-01",sep=""))
full_data_news
full_news_data
full_news_data[datetime == ymd("2011-03-01")]
full_news_data[datetime == ymd("2011-03-01"),]
full_news_data[full_news_data$datetime == ymd("2011-03-01"),]
full_news_data[category=='the',]
N_NEWS_ARTICLES <- length(unique(news_df$unique_id))
N_NEWS_ARTICLES
news_df
ctm
length(unique(with(ctm,paste0(articleNumber,ymd_date))))
ctm
ctm$unique_id <- paste0(ctm$articleNumber,ctm$date)
ctm
length(unique(ctm$unique_id))
N_NEWS_ARTICLES
ctm$unique_id <- paste0(ctm$articleNumber,ctm$date)
N_NEWS_ARTICLES <- length(unique(ctm$unique_id))
N_NEWS_ARTICLES
save(full_news_data,N_NEWS_ARTICLES,ALL_NEWS_CATEGORIES,ALL_NEWS_COUNTRIES,ALL_NEWS_TIMES, file="result_data/full_news_data.rdata")
##all this data was determined via casos_get_twitter_term_nets.py
DATA_DIR <- "raw_data"
library(lubridate)
library(dplyr)
library(tidyr)
library(data.table)
library(ggplot2)
library(bit64)
remove_countries <- c("sudan","qatar")
TERMS_TO_REMOVE <- c("im","saudi","pkk","dr")
inv_logis <- function(x){return(log(x/(1-x)))}
concept_to_category <- fread(file.path(DATA_DIR,"goldstone_to_category.tsv"),header=F)
setnames(concept_to_category,c("concept","category"))
concept_to_category$concept <- tolower(concept_to_category$concept)
###USER COUNT STUFF
#the number of users per country per month that tweeted at least once.
##loc_type is how we determined country (full == both, geo, text)
user_count <- fread(file.path(DATA_DIR,"twitter_usercounts.tsv"))
user_count <- user_count[,sum(V3),by=c("V1","V2","V4")]
setnames(user_count, c("country","date","loc_type","count"))
user_count <- user_count[!country %in% remove_countries]
user_count$datetime <- ymd(paste(user_count$date,"-01",sep=""))
user_count$date <- NULL
####TERM COUNT STUFF
term_count <- fread(file.path(DATA_DIR, "twitter_termcounts.tsv"))
##all this data was determined via casos_get_twitter_term_nets.py
DATA_DIR <- "raw_data"
library(lubridate)
library(dplyr)
library(tidyr)
library(data.table)
library(ggplot2)
library(bit64)
remove_countries <- c("sudan","qatar")
TERMS_TO_REMOVE <- c("im","saudi","pkk","dr")
inv_logis <- function(x){return(log(x/(1-x)))}
concept_to_category <- fread(file.path(DATA_DIR,"goldstone_to_category.tsv"),header=F)
setnames(concept_to_category,c("concept","category"))
concept_to_category$concept <- tolower(concept_to_category$concept)
###USER COUNT STUFF
#the number of users per country per month that tweeted at least once.
##loc_type is how we determined country (full == both, geo, text)
user_count <- fread(file.path(DATA_DIR,"twitter_usercounts.tsv"))
user_count <- user_count[,sum(V3),by=c("V1","V2","V4")]
setnames(user_count, c("country","date","loc_type","count"))
user_count <- user_count[!country %in% remove_countries]
user_count$datetime <- ymd(paste(user_count$date,"-01",sep=""))
user_count$date <- NULL
####TERM COUNT STUFF
term_count <- fread(file.path(DATA_DIR, "twitter_termcounts.tsv"))
setnames(term_count, c("tw_id","datetime","uid","concept","raw","country","loc_type"))
###see if anything weird showed up... definitely "im", "pkk" and "saudi", which is used to capture tweets.
#Get rid of them
ck_bad_raw <- term_count[,length(unique(uid)),by=c("raw","concept")]%>% arrange(-V1)
term_count <- term_count[!raw %in% TERMS_TO_REMOVE,]
term_count$datetime <- ymd(term_count$datetime)
####***only using 2011 -> beginning of 2013 for sampling reasons
term_count <- term_count[datetime > ymd("2011-01-31") & datetime < ymd("2013-03-01"),]
term_count$md <- paste(year(term_count$datetime),month(term_count$datetime),sep="-")
term_count$loc_type <- tolower(term_count$loc_type)
term_count$concept <- tolower(term_count$concept)
term_count <- merge(term_count,concept_to_category,by="concept",all.x=T)
##See if there are any concepts we can just ignore because they are so sparse
ck_lame_cat <- term_count[,length(unique(uid)),by=c("category")] %>% arrange(-V1)
term_count <- term_count[!is.na(category)] #things we clean post-hoc from concept -> category list
##Lets toss the low-lying categories - < 5000 users out of 800K is pretty uninteresting, at least for right now
term_count <- term_count[category %in% ck_lame_cat[V1 > 10000,]$category]
ck_lame_cat
##all this data was determined via casos_get_twitter_term_nets.py
DATA_DIR <- "raw_data"
library(lubridate)
library(dplyr)
library(tidyr)
library(data.table)
library(ggplot2)
library(bit64)
remove_countries <- c("sudan","qatar")
TERMS_TO_REMOVE <- c("im","saudi","pkk","dr")
inv_logis <- function(x){return(log(x/(1-x)))}
concept_to_category <- fread(file.path(DATA_DIR,"goldstone_to_category.tsv"),header=F)
setnames(concept_to_category,c("concept","category"))
concept_to_category$concept <- tolower(concept_to_category$concept)
###USER COUNT STUFF
#the number of users per country per month that tweeted at least once.
##loc_type is how we determined country (full == both, geo, text)
user_count <- fread(file.path(DATA_DIR,"twitter_usercounts.tsv"))
user_count <- user_count[,sum(V3),by=c("V1","V2","V4")]
setnames(user_count, c("country","date","loc_type","count"))
user_count <- user_count[!country %in% remove_countries]
user_count$datetime <- ymd(paste(user_count$date,"-01",sep=""))
user_count$date <- NULL
####TERM COUNT STUFF
term_count <- fread(file.path(DATA_DIR, "twitter_termcounts.tsv"))
setnames(term_count, c("tw_id","datetime","uid","concept","raw","country","loc_type"))
###see if anything weird showed up... definitely "im", "pkk" and "saudi", which is used to capture tweets.
#Get rid of them
ck_bad_raw <- term_count[,length(unique(uid)),by=c("raw","concept")]%>% arrange(-V1)
term_count <- term_count[!raw %in% TERMS_TO_REMOVE,]
term_count$datetime <- ymd(term_count$datetime)
####***only using 2011 -> beginning of 2013 for sampling reasons
term_count <- term_count[datetime > ymd("2011-01-31") & datetime < ymd("2013-03-01"),]
term_count$md <- paste(year(term_count$datetime),month(term_count$datetime),sep="-")
term_count$loc_type <- tolower(term_count$loc_type)
term_count$concept <- tolower(term_count$concept)
term_count <- merge(term_count,concept_to_category,by="concept",all.x=T)
term_count[is.na(category)]
concept_to_category
'we'%in% concept_to_category$concept
