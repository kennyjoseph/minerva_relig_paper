
Intro stuff
===========
```{r gen-data, cache=TRUE, echo=FALSE, results='hide',message=FALSE}
DATA_DIR <- "raw_data"

library(lubridate)
library(dplyr)
library(tidyr)
library(data.table)
library(ggplot2)
library(bit64)

inv_logis <- function(x){return(log(x/(1-x)))}

```

Load data
======================

The process to get this data is as follows:
1. Run ```casos_get_twitter_term_nets.py``` on the data
2. Run ```casos_get_news_term_nets.py``` on the data
3. Run  ```transform_raw_news_data.R``` which will save out ```result_data/full_news_data.rdata``` and ```results_data/news_data.rdata```
4. Run ```transform_raw_twitter_data.R```, which will save out  ```result_data/full_twitter_data.rdata``` and ```results_data/twitter_data.rdata```
5. Run ```get_simplified_nu_value.R```, which will finally save out ```result_data/final_data.rdata```. It'd probably be good to keep this last step in the markdown data but moving it elsewhere and just loading in the data lets me not have to load a ridiculous amount of data into the process in this script

```{r gen-data, cache=TRUE, results='hide',message=FALSE}
load("result_data/final_data.rdata")
```


Estimation for Eisenstein paper
========================================================
The model in Eisenstein et al's work is as follows:

$$
\begin{align}
c_{w,r,t} &\sim \mathrm{Binomial}(s_{r,t}, \mathrm{Logistic}(\eta_{w,r,t} + v_{w,t} + \mu_{r,t})) \\
\eta_{w,r,t} &\sim \mathrm{N}(\sum_{r'} a_{r',r}\eta_{w,r',t-1}, \sigma^2_{w,r}) \\
\end{align}
$$

Here the subscripts represent the following: $w$ is the word, $r$ is the region, $t$ is the timestep. The parameter $c_{w,r,t}$ is the number of unique individuals in region $r$ at time $t$ who used word $w$, $s_{r,t}$ is the number of users in $r$ at $t$ who tweeted at least once. The parameter $\eta_{w,r,t}$ is the activation for word $w$ at time $t$ in region $r$. The parameter $v_{w,t}$ is the overall activation of $w$ at $t$, and $\mu_{r,t}$ is the activation of the region $r$ at time $t$.  Finally, the parameter $a_{r',r}$ is the lagged influence of region $r'$ on the activation of all words in region $r$.

Here, we do some EDA:
```{r fig.width=12, fig.height=8, cache=TRUE}
fd <- final_data[category %in% c("nationality","tribe")]
fd$prop <- ifelse(fd$s_wrt == 0, 0, fd$c_wrt/fd$s_wrt)
ggplot(fd, aes(datetime,prop,color=category,shape=type)) + geom_point() + facet_wrap(~country, scales="free_y") + geom_line() + ylab("Proportion of Users Used it") + xlab("Month") + theme(axis.text.x=element_text(angle=45,hjust=1))
```

```{r fig.width=12, fig.height=8, cache=TRUE}
fd2 <- fd[category %in% c("nationality","tribe"),
         c("country","datetime","category","c_wrt","type"),with=F] %>%
        spread(category,c_wrt)

ggplot(fd2, aes(datetime,log((nationality+.1)/(tribe+.1)),color=type)) + geom_point() + facet_wrap(~country, scales="free_y") + geom_line() + ylab("Log-odds of nationality vs. tribe") + xlab("Month") + theme(axis.text.x=element_text(angle=45,hjust=1)) + geom_hline(y=0)
```


Because of the lagged influence assumption, the model proposed is a latent vector autoregression movel, where $\eta_{w,r,t}$ and $A$, the matrix of $a_{r',r}$ values, are the parameters of interest, and $v$ and $\mu$ are control parameters that partial out effects of oversampling of particular words at particilar times and particular regions at particular times. The full autoregression model can be specified as a Markov model with an *observation model* (on $c$) and a *dynamics*  model on $\nu$. The model we thus wish to estimate is:

$$ P(\eta,c|s; A, \sigma^2,\mu,v) = P(c | \eta, s; \mu, v) P(\eta; A)$$

In order to do so, we adopt the same estimation process as was used by Eisenstein.  We first consider the estimation of $v$ and $\mu$ for each word assuming $\eta$ is 0.  To do so, we utilize a stepwise proceedure.  First, we obtain a simplified $\bar{v_w}$ as the inverse logistic function ($log(\frac{x}{1-x}$) of the total proportion of users that utilized word $w$ across all time steps. This is done in the file ```get_simplified_nu_value.R``` and is called ```simp_v_est``` in ```final_data```

Using this, we now would like to compute the MLE of each $\mu_{r,t}$ using this as $v_{w,t}$ (and again, setting all $\eta = 0$).  Below I derive the MLE for a single time point $t$ and a single word $w$ (subscripts thus implicit), a step absent presumably due to simplicitly/space from the original article.  In order to do so, I have to optimize the following (note that maximizing the likelihood is the same as minimizing the negative log-likelihood):

$$
\begin{align}
  \hat{\mu_r} &= \mathrm{argmax}_{\mu} \prod_w P(c_w | s; \mu, v_w) \\
  &= \mathrm{argmax}_{\mu} \prod_w   ({}^{c_w}_{s}) \frac{1}{1+\exp(-(v_w+\mu))}^{c_w} (1- \frac{1}{ 1+\exp(-(v_w+\mu))})^{s-c_w} \\
  &= \mathrm{argmin}_{\mu} - \sum_w \log({}^{c_w}_{s})  + \log(\frac{1}{1+ \exp(-(v_w+\mu))}^{c_w}) + \log( (1- \frac{1}{ 1+ \exp(-(v_w+\mu))})^{s-c_w}) \\
  &= \mathrm{argmin}_{\mu} - \sum_w \log({}^{c_w}_{s})  - c_w \log(1+ \exp(-(v_w+\mu)) + (s-c_w) \log (1- \frac{1}{ 1+ \exp(-(v_w+\mu))})
\end{align}
$$

We can now take the derivative of the above expression and use it for a gradient descent approach to maximizing the function above. The gradient for the negative log-likelihood for $\mu$ can be derived as follows:

$$
\begin{align}
 \frac{\partial}{\partial \mu} \sum_w \log({}^{c_w}_{s})  - c_w \log(1+ \exp(-(v_w+\mu))) + (s-c_w) \log (1- \frac{1}{ 1+ \exp(-(v_w+\mu))}) \\
  =  \sum_w \frac{(c_w-s) \exp(v_w+\mu)}{ 1+ \exp(v_w+\mu)} +\frac{c_w}{1+ \exp(v_w+\mu)}  
\end{align}
$$

We use this as the input for a gradient descent proceedure to estimate $\mu$. We then plug these estimates back in to perform the estimation for $v$.  Code for this is below:


```{r, cache=TRUE, results='hide'}

STEP_SIZE = .00001 # step size
PRECISION = .000000001

##generic gradient, the estimation doesn't change much between the two variables
gradient_function <- function(old_val, data, expval){
  return(-sum(
      ((data$c_wrt-data$s_wrt) * expval/(1+expval)) +
      (data$c_wrt /(1+expval))
    )
  )
}

##dummy data
final_data$mu_est <- -99
final_data$v_est <- -99

## Gradient descent for mu
for(type_var in c("NEWS","TWITTER")){
  for(country_var in ALL_COUNTRIES) {
    for(time_var in ALL_TIMES){
      data <- final_data[final_data$type==type_var &
                        country == country_var & 
                        final_data$datetime == time_var]
      mu_old = 0
      mu_new = 1
      i = 0
      while(i < 10000000  & abs(mu_old - mu_new) > PRECISION ){
        i <- i + 1
        mu_old = mu_new
        expval <- exp(data$simp_v_w + mu_old)
        grad <- gradient_function(mu_old, data, expval)
        mu_new = mu_old - STEP_SIZE * grad
      }
      print(paste("New minimum for time", time_var, 
                  " in country", country_var, " is ", mu_new))
      final_data[country == country_var & 
                  final_data$datetime == time_var &
                  type==type_var ]$mu_est <- mu_new
    }
  }
}

##Gradent descent for v
for(type_var in c("NEWS","TWITTER")){
  for(category_var in ALL_CATEGORIES) {
    for(time_var in ALL_TIMES){
      data <- final_data[category == category_var & 
                        final_data$datetime == time_var &
                        type == type_var]
      v_old = 0
      v_new = 1
      i = 0
      while(i <10000000  & abs(v_old - v_new) > PRECISION ){
        i <- i + 1
        v_old = v_new
        expval <- exp(data$mu_est + v_old)
        grad <- gradient_function(v_old, data, expval)
        v_new = v_old - STEP_SIZE * grad
      }
      print(paste("New minimum for time", time_var, 
                  " in category", category_var, " is ", v_new))
      final_data[category == category_var & 
                  final_data$datetime == time_var&
                  type==type_var]$v_est <- v_new
    }
  }
}


```


Check out the $$v$$ estimates:

```{r fig.width=12, fig.height=8, cache=TRUE}
ggplot(final_data,aes(datetime,v_est,color=type)) + geom_point() + geom_line() + facet_wrap(~category)
```

Check out the $$\mu$$ estimates:

```{r fig.width=12, fig.height=8, cache=TRUE}

ggplot(final_data,aes(datetime,mu_est,color=type)) + geom_point() + geom_line() + facet_wrap(~country)

```


Now that we have our estimates for $\mu$ and $v$, we will estimate values for $\eta_{w,r,t}$ and $\sigma^2_{w,r}$. To do so, we introduce a diagonal $A$ matrix $\tilde{A_w}$ for each word $w$, which we will use in place of $A$. Because the observation model is non-Gaussian, we cannot use the Kalman Filter algorithm to perform inference. Instead, we must resort to sampling methods to perform approximate draws from the distribution of the $\eta$ s over time. We can then use maximum likelihood estimation to update the diagonal of $\tilde{A_w}$ and $\sigma^2_{w,r}$.  We then update expectations for $\tilde{A_w}$ and $\sigma^2_{w,r}$ and iterate again to generate a draw from an updated version of the distribution for $\eta_{w,r,t}$. 

To accomplish this iterative process, we use Monte-Carlo EM. In the E step, we get an expectation for $\eta$ using Forwards-Filtering Backwards Sampling (FFBS). Eistenstein et al. do not specify an initial distribution for $\eta_0$, so we draw from $\mathrm{N}(0, \sigma^2)$ and also assume an initial $\sigma^2=1$.  From here, we run a simple particle filter with the proposal distribution $Q(x)$ equal to the transition distribution. We can thus construct weights for each sample $\eta_{w,r,t}^{(m)}$ recursively as:

$$
\begin{equation} 
\omega_{w,r,t}^{(m)} = \omega_{w,r,t-1}^{(m)} * P(c{w,r,t} | \eta_{w,r,t}, s_{w,t}; \mu_{r,t}, v_{w,t}) \\
\end{equation}
$$

With FFBS using a simple particle filter on the forward pass, we simply construct these weights. On the backwards pass, we will actually draw our samples for $eta_{w,r,t}$ using these weights.  This completes the E step.

The M step updates the MLEs for $\tilde{A_w}$ and $\sigma^2_{w,r}$. The update for $\tilde{A_w}$ is simply determined via least-squares estimation. The MLE for $\sigma^2_{w,r}$ can be solved in closed form, $\frac{1}{T}\sum_t^T(\eta_{w,r,t} - \tilde{a_w,r}\eta_{w,r,t-1})$.


```{r}
N_SAMPLES_PER_TIME_PERIOD <- 1000
N_TIME_PERIODS <- length(ALL_TIMES)
THRESHOLD <- .01
MAX_ITERATION <- 20

##Slow but right
get_resampled <- function(t, eta_sample_set, sigma, data, 
                          times=ALL_TIMES,
                          n_samples=N_SAMPLES_PER_TIME_PERIOD){
  ##get likelihoods
  
  date_data <- data[data$datetime == times[t]]
  weights <- rep(NA,n_samples)
  for(sample_num in 1:n_samples){
    p_param <- 1/(1+ exp(-(date_data$mu_est+date_data$v_est+eta_sample_set[sample_num])))
    weights[sample_num] <- dbinom(date_data$c_wrt,date_data$s_wrt, p_param)
  }
  weights <- weights/sum(weights)
  #print(weights[weights > .001])
  ##it seems wise to resample here:
  sampled <- sample(eta_sample_set,size=n_samples,replace=T,prob=weights)
  #print(paste("unique samples: ", length(unique(sampled))/n_samples))
  return(sampled)
  
}

###MCEM alg.


#interesting_categories <- c("violence","war","tribe","terrorist_org","nationality","protest")#,"human_rights",

results <- data.table(expand.grid(country=ALL_COUNTRIES,
                       category=ALL_CATEGORIES,
                       type=c("NEWS","TWITTER"),
                       a=-99,sigma=-99,stringsAsFactors=F))
eta_samples_list <- list()

##for each word
for(type_var in c("TWITTER","NEWS")){
  for(category_var in ALL_CATEGORIES){
    ##A is global across the word
    ##sigma is defined per country
    A <- 1.5
    old_A <- -1
    sigma <- as.list(sapply(ALL_COUNTRIES,function(l){return(3)}))
    old_sigma <- as.list(sapply(ALL_COUNTRIES,function(l){return(-1)}))
    turn_iter <- 0
    
    A_estimation_matrix <- data.frame(x=c(),y=c())
    
    ##while not convergence or max iterations
    while( (any(abs(unlist(old_sigma)-unlist(sigma)) > THRESHOLD) | abs(A-old_A) > THRESHOLD) & 
             turn_iter < MAX_ITERATION ) {
      
      per_cat_eta_samples_list <- list()
      
      turn_iter <- turn_iter + 1
      print(paste0("turn: ",turn_iter))
      
      ##get samples for each country
      for(country_var in ALL_COUNTRIES){  
        print(paste("STARTING:::::::: ",country_var,category_var))
        data <- final_data[country == country_var & category == category_var & type == type_var]
        
        country_sigma <- sigma[[country_var]]
        
        ##E STEP
          ##initialize with p0
          eta_samples <- matrix(-1,nrow=length(ALL_TIMES),ncol=N_SAMPLES_PER_TIME_PERIOD)
          init_date_data <- data[data$datetime == ALL_TIMES[1]]
          init_p <- init_date_data$c_wrt/init_date_data$s_wrt
          mean_param <- log(init_p/(1-init_p)) - init_date_data$v_est - init_date_data$mu_est
          eta_samples[1,] <- rnorm(N_SAMPLES_PER_TIME_PERIOD,mean_param,country_sigma)
        
          ##Forward Filtering (SIR)
          for(t in 2:N_TIME_PERIODS){
            #print(paste0("\t",t))
            eta_samples[t,] <- rnorm(n=N_SAMPLES_PER_TIME_PERIOD,
                                     mean=A*eta_samples[t-1,],
                                     sd=rep(country_sigma,N_SAMPLES_PER_TIME_PERIOD))
            eta_samples[t,] <- get_resampled(t, eta_samples[t,],country_sigma,data)
          }
          ##Backwards Smoothing
          for(t in seq(N_TIME_PERIODS,1,-1)){
            eta_samples[t, ] <- sample(eta_samples[t,],size=N_SAMPLES_PER_TIME_PERIOD,replace=T)
          }
      
        ##prep for M step of A, M step for sigma
          ##Add the samples from here into the eventual re-estimation of 
          a_est_for_country <- data.frame(x=c(),y=c())
          for(time_it in 1:(length(ALL_TIMES)-1)){
            a_est_for_country <- rbind(a_est_for_country, 
                                       data.frame(x=mean(eta_samples[time_it,]),y=mean(eta_samples[time_it+1,])))
          }
          A_estimation_matrix <- rbind(A_estimation_matrix, a_est_for_country)
        
          ##Estimate sigma
          m2 <- data.frame(x=c(),y=c())
          for(time_it in 2:(length(ALL_TIMES))){
            m2 <- rbind(m2, data.frame(x=mean(eta_samples[time_it,]),y=old_A*mean(eta_samples[time_it-1,])))
          } 
          old_sigma[[country_var]] <- country_sigma
          sigma[[country_var]] <- sqrt(sum((m2$x-m2$y)^2)/nrow(m2))
          print(paste("Country:", country_var, 
                      "Type: ", type_var,
                      "Old sigma:", old_sigma[[country_var]], 
                      "New sigma:", sigma[[country_var]], 
                      "diff:", abs(old_sigma[[country_var]]-sigma[[country_var]]) ))  
          ##save the samples
          esl <- list(x=eta_samples)
          names(esl) <- paste(type_var,country_var,category_var,sep="_")
          per_cat_eta_samples_list <- c(per_cat_eta_samples_list,esl)
          results[country==country_var & category==category_var & type ==type_var, "sigma"] <- sigma[[country_var]]
      } ##end for country
      
      ##UPDATE A
      a_lm <- lm(y~x-1,A_estimation_matrix)
      old_A <- A
      A <- as.numeric(a_lm$coefficients[1])
      print(paste("Old A:", old_A, " New A:", A, "diff:", abs(old_A-A) ))
      results[category==category_var & type == type_var]$a <- A
      
    } ##end while iter
    eta_samples_list <- c(eta_samples_list,per_cat_eta_samples_list)
    save(eta_samples_list, file="result_data/eta_samples_list.rdata")
    save(results, file="result_data/results.rdata")
  } ## category for loop

} ## type for loop


results <- results[paste(type,country,category,sep="_") %in% names(eta_samples_list)]
eta_res <- results[,as.list(apply(eta_samples_list[[paste(type,country,category,sep="_")]],1,mean)),by=c("type","country","category","a","sigma")]

time_df <- data.frame(variable=paste0("V",1:23), date=sort(ALL_NEWS_TIMES[ALL_NEWS_TIMES %in% ALL_TIMES]))
eta_res <- melt(eta_res, id.vars=c("type","country","category","a","sigma"))
eta_res <- merge(eta_res, time_df,by="variable")
eta_res$variable <- NULL

ggplot(eta_res[category=='violence',], aes(date,value,color=type)) + geom_point() + geom_line() + facet_wrap(~country)




```

