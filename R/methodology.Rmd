
Intro stuff
===========
```{r gen-data, cache=TRUE, echo=FALSE, results='hide',message=FALSE}
DATA_DIR <- "raw_data"

library(lubridate)
library(dplyr)
library(tidyr)
library(data.table)
library(ggplot2)
library(bit64)

inv_logis <- function(x){return(log(x/(1-x)))}

```

Load data
======================

The process to get this data is as follows:
1. Run ```casos_get_twitter_term_nets.py``` on the data
2. Run ```casos_get_news_term_nets.py``` on the data
3. Run  ```transform_raw_news_data.R``` which will save out ```result_data/full_news_data.rdata``` and ```results_data/news_data.rdata```
4. Run ```transform_raw_twitter_data.R```, which will save out  ```result_data/full_twitter_data.rdata``` and ```results_data/twitter_data.rdata```
5. Run ```get_simplified_nu_value.R```, which will finally save out ```result_data/final_data.rdata```. It'd probably be good to keep this last step in the markdown data but moving it elsewhere and just loading in the data lets me not have to load a ridiculous amount of data into the process in this script

```{r gen-data, cache=TRUE, results='hide',message=FALSE}
load("result_data/final_data.rdata")
```


Estimation for Eisenstein paper
========================================================
The model in Eisenstein et al's work is as follows:

$$
\begin{align}
c_{w,r,t} &\sim \mathrm{Binomial}(s_{r,t}, \mathrm{Logistic}(\eta_{w,r,t} + v_{w,t} + \mu_{r,t})) \\
\eta_{w,r,t} &\sim \mathrm{N}(\sum_{r'} a_{r',r}\eta_{w,r',t-1}, \sigma^2_{w,r}) \\
\end{align}
$$

Here the subscripts represent the following: $w$ is the word, $r$ is the region, $t$ is the timestep. The parameter $c_{w,r,t}$ is the number of unique individuals in region $r$ at time $t$ who used word $w$, $s_{r,t}$ is the number of users in $r$ at $t$ who tweeted at least once. The parameter $\eta_{w,r,t}$ is the activation for word $w$ at time $t$ in region $r$. The parameter $v_{w,t}$ is the overall activation of $w$ at $t$, and $\mu_{r,t}$ is the activation of the region $r$ at time $t$.  Finally, the parameter $a_{r',r}$ is the lagged influence of region $r'$ on the activation of all words in region $r$.

Here, we do some EDA:
```{r fig.width=12, fig.height=8, cache=TRUE}
fd <- final_data[category %in% c("nationality","tribe")]
fd$prop <- ifelse(fd$s_wrt == 0, 0, fd$c_wrt/fd$s_wrt)
ggplot(fd, aes(datetime,prop,color=category,shape=type)) + geom_point() + facet_wrap(~country, scales="free_y") + geom_line() + ylab("Proportion of Users Used it") + xlab("Month") + theme(axis.text.x=element_text(angle=45,hjust=1))
```

```{r fig.width=12, fig.height=8, cache=TRUE}
fd2 <- fd[category %in% c("nationality","tribe"),
         c("country","datetime","category","c_wrt","type"),with=F] %>%
        spread(category,c_wrt)

ggplot(fd2, aes(datetime,log((nationality+.1)/(tribe+.1)),color=type)) + geom_point() + facet_wrap(~country, scales="free_y") + geom_line() + ylab("Log-odds of nationality vs. tribe") + xlab("Month") + theme(axis.text.x=element_text(angle=45,hjust=1)) + geom_hline(y=0)
```


Because of the lagged influence assumption, the model proposed is a latent vector autoregression movel, where $\eta_{w,r,t}$ and $A$, the matrix of $a_{r',r}$ values, are the parameters of interest, and $v$ and $\mu$ are control parameters that partial out effects of oversampling of particular words at particilar times and particular regions at particular times. The full autoregression model can be specified as a Markov model with an *observation model* (on $c$) and a *dynamics*  model on $\nu$. The model we thus wish to estimate is:

$$ P(\eta,c|s; A, \sigma^2,\mu,v) = P(c | \eta, s; \mu, v) P(\eta; A)$$

In order to do so, we adopt the same estimation process as was used by Eisenstein.  We first consider the estimation of $v$ and $\mu$ for each word assuming $\eta$ is 0.  To do so, we utilize a stepwise proceedure.  First, we obtain a simplified $\bar{v_w}$ as the inverse logistic function ($log(\frac{x}{1-x}$) of the total proportion of users that utilized word $w$ across all time steps. This is done in the file ```get_simplified_nu_value.R``` and is called ```simp_v_est``` in ```final_data```

Using this, we now would like to compute the MLE of each $\mu_{r,t}$ using this as $v_{w,t}$ (and again, setting all $\eta = 0$).  Below I derive the MLE for a single time point $t$ and a single word $w$ (subscripts thus implicit), a step absent presumably due to simplicitly/space from the original article.  In order to do so, I have to optimize the following (note that maximizing the likelihood is the same as minimizing the negative log-likelihood):

$$
\begin{align}
  \hat{\mu_r} &= \mathrm{argmax}_{\mu} \prod_w P(c_w | s; \mu, v_w) \\
  &= \mathrm{argmax}_{\mu} \prod_w   ({}^{c_w}_{s}) \frac{1}{1+\exp(-(v_w+\mu))}^{c_w} (1- \frac{1}{ 1+\exp(-(v_w+\mu))})^{s-c_w} \\
  &= \mathrm{argmin}_{\mu} - \sum_w \log({}^{c_w}_{s})  + \log(\frac{1}{1+ \exp(-(v_w+\mu))}^{c_w}) + \log( (1- \frac{1}{ 1+ \exp(-(v_w+\mu))})^{s-c_w}) \\
  &= \mathrm{argmin}_{\mu} - \sum_w \log({}^{c_w}_{s})  - c_w \log(1+ \exp(-(v_w+\mu)) + (s-c_w) \log (1- \frac{1}{ 1+ \exp(-(v_w+\mu))})
\end{align}
$$

We can now take the derivative of the above expression and use it for a gradient descent approach to maximizing the function above. The gradient for the negative log-likelihood for $\mu$ can be derived as follows:

$$
\begin{align}
 \frac{\partial}{\partial \mu} \sum_w \log({}^{c_w}_{s})  - c_w \log(1+ \exp(-(v_w+\mu))) + (s-c_w) \log (1- \frac{1}{ 1+ \exp(-(v_w+\mu))}) \\
  =  \sum_w \frac{(c_w-s) \exp(v_w+\mu)}{ 1+ \exp(v_w+\mu)} +\frac{c_w}{1+ \exp(v_w+\mu)}  
\end{align}
$$

We use this as the input for a gradient descent proceedure to estimate $\mu$. We then plug these estimates back in to perform the estimation for $v$.  Code for this is in the file: ```get_mu_v_estimate.R```

Check out the $$v$$ estimates:

```{r fig.width=12, fig.height=8, cache=TRUE}
load("result_data/final_data_w_mu_v.rdata")
ggplot(final_data,aes(datetime,v_est,color=type)) + geom_point() + geom_line() + facet_wrap(~category)
```

Check out the $$\mu$$ estimates:

```{r fig.width=12, fig.height=8, cache=TRUE}

ggplot(final_data,aes(datetime,mu_est,color=type)) + geom_point() + geom_line() + facet_wrap(~country)

```


Now that we have our estimates for $\mu$ and $v$, we will estimate values for $\eta_{w,r,t}$ and $\sigma^2_{w,r}$. To do so, we introduce a diagonal $A$ matrix $\tilde{A_w}$ for each word $w$, which we will use in place of $A$. Because the observation model is non-Gaussian, we cannot use the Kalman Filter algorithm to perform inference. Instead, we must resort to sampling methods to perform approximate draws from the distribution of the $\eta$ s over time. We can then use maximum likelihood estimation to update the diagonal of $\tilde{A_w}$ and $\sigma^2_{w,r}$.  We then update expectations for $\tilde{A_w}$ and $\sigma^2_{w,r}$ and iterate again to generate a draw from an updated version of the distribution for $\eta_{w,r,t}$. 

To accomplish this iterative process, we use Monte-Carlo EM. In the E step, we get an expectation for $\eta$ using Forwards-Filtering Backwards Sampling (FFBS). Eistenstein et al. do not specify an initial distribution for $\eta_0$, so we draw from $\mathrm{N}(0, \sigma^2)$ and also assume an initial $\sigma^2=1$.  From here, we run a simple particle filter with the proposal distribution $Q(x)$ equal to the transition distribution. We can thus construct weights for each sample $\eta_{w,r,t}^{(m)}$ recursively as:

$$
\begin{equation} 
\omega_{w,r,t}^{(m)} = \omega_{w,r,t-1}^{(m)} * P(c{w,r,t} | \eta_{w,r,t}, s_{w,t}; \mu_{r,t}, v_{w,t}) \\
\end{equation}
$$

With FFBS using a simple particle filter on the forward pass, we simply construct these weights. On the backwards pass, we will actually draw our samples for $eta_{w,r,t}$ using these weights.  This completes the E step.

The M step updates the MLEs for $\tilde{A_w}$ and $\sigma^2_{w,r}$. The update for $\tilde{A_w}$ is simply determined via least-squares estimation. The MLE for $\sigma^2_{w,r}$ can be solved in closed form, $\frac{1}{T}\sum_t^T(\eta_{w,r,t} - \tilde{a_w,r}\eta_{w,r,t-1})$.

This is implemented in the file ```get_final_eta_estimates.R```.

```{r}

blah <- list(); for(f in Sys.glob("result_data/*_eta_samples.rdata")){ load(f); blah <- c(blah,per_cat_eta_samples_list) }
results <- results[paste(type,country,category,sep="_") %in% names(eta_samples_list)]
eta_res <- results[,as.list(apply(eta_samples_list[[paste(type,country,category,sep="_")]],1,mean)),by=c("type","country","category","a","sigma")]

time_df <- data.frame(variable=paste0("V",1:23), date=sort(ALL_NEWS_TIMES[ALL_NEWS_TIMES %in% ALL_TIMES]))
eta_res <- melt(eta_res, id.vars=c("type","country","category","a","sigma"))
eta_res <- merge(eta_res, time_df,by="variable")
eta_res$variable <- NULL

ggplot(eta_res[category=='violence',], aes(date,value,color=type)) + geom_point() + geom_line() + facet_wrap(~country)




```

