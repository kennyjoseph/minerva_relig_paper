\section{Data}

The present work focuses on discussions of  N human-curated topical categories in large corpora of news and Twitter data.  We focus on 15 countries in the Arab World, plus data relevant to Iran.  The countries of interest are given in Figure~\ref{X}. Below, we describe the Twitter and news datasets utilized.  For more information on the Twitter data, we refer the reader to \cite{}.  For more information on the news data, we refer the reader to \cite{}.  Note that we use a subset of the Twitter data described in previous work here, and an expanded news dataset collected in the same fashion as the data used in previous work.

\subsection{Twitter data}

The present work utilizes a corpora of approximately 70M tweets collected between February, 2011 and February, 2013.  The data was collected from a set of two sources. The first utilized a variety of approaches, including geospatial bounding boxes, keyword searches and searches for specific Twitter users, to capture data that data collectors deemed relevant to the Arab Spring.  As the data was obtained from an outside source, an exact listing of terms and spatial regions from which this set of data was collected is unavailable to us.  However, most of the collection is known to have focused on events relevant to Egypt, Libya, Syria, Tunisia and Yemen.  The second source of data is obtained from a 10\% sample of all Twitter data made available to members of our university.  From this sample, we extract all tweets that were geo-tagged from a location inside the nations of interest. Any geo-tagged tweet from a particular country is assumed to be relevant to that country.  In addition, any tweet that uses a country's name, in Arabic or in English, or a tweet that uses the name of any of the country's five most populous cities in English or in Arabic, is assumed to be relevant to that country.  Note, then, that the same tweet may be considered relevant to multiple countries.  

\subsection{News data}

  
\section{Methodology}

There are three major parts to the methodology completed in this paper.  We first cover the manner in which we determined the themes, or categories, of interest to us and how we selected sets of terms used as a proxy for the discussion of each of these categories. We then describe the methodology we utilize to determine the extent to which each category was being discussed in different countries over time. Finally, we detail methods used to analyze the output of this model in order to better understand our three research questions.

\subsection{Categories}

As the data we use only covers periods during and after the first revolts, the categories we considered in our analysis were chiefly focused on identifying discussions on topics relevant to the spread of the Arab Spring and the extent to which revolutions in different nations succeeded.  After iterating on possible categories, we first settled on twenty categories that we were interested in better understanding. Following a decision on the categories of interest, our next task was to determine a set of terms\footnote{Note that a term may comprise multiple words, e.g. ``Michael Jordan'' might be a single term for the category ``sport''} that, if mentioned in a tweet or news article, the authors agreed pointed to a discussion pertinent to that topic.  This list was refined by the authors until all agreed on the set of terms relevant to each category. Finally, we searched through every tweet and news article in our dataset for these terms.  Due to the relatively data-hungry nature of the statistical model described in the following section, we eventually chose to ignore any category that was found in fewer than 5,000 news articles or 5,000 tweets.  This left us with a final set of twelve categories that we considered in our analysis.

\begin{table}[ht]
\begin{tabularx}{\textwidth}{| m{3.5cm} | m{4.5cm} | m{1.5cm} | X |}
	\hline 
	{\bf Category} & {\bf Reason for Use} & {\bf Num. Terms} & {\bf Representative Terms} \\ \hline
	  Terrorist Orgs (terrorist\_org) & Indicator of instability & 3619 & Al-qaeda, terrorism \\ \hline
	   Adaptation & Indicator of change & 130 & adjustments,  amendment \\ \hline
      Protest &  Indicator of unrest & 19 & protests, demonstrations \\ \hline
   Violence & Indicator of unrest & 39 & violence, violent \\ \hline
    War & Indicator of unrest & 46 &  war, wartime \\ \hline
    National identities (nationality) & \cite{goldstone_cross-class_2011}& 245 & Algerian, Syrian \\ \hline  
   Youth & \cite{goldstone_cross-class_2011} &   10 & young person, youngster \\ \hline
   Non-national identities (tribe) & \cite{goldstone_cross-class_2011} & 1489 & Shi'a, Sunni \\ \hline
  Profession & \cite{goldstone_cross-class_2011} &  395 & teacher, textile worker \\ \hline
   Noise 1 (stop4) & Check for spuriousness & 14 & good, person, problem  \\ \hline
 Noise 2 (stop5) & Check for spuriousness & 16 & good, person, problem \\ \hline
 Noise 3 (stop6) & Check for spuriousness & 14 & good, person, problem\\ \hline
\end{tabularx}
	\caption{Categories Used}
	\label{tab:categories}
\end{table}

Table~\ref{tab:categories} presents the set of categories used in the present work, along with a description of why they were chosen, the number of terms that map to the category and a set of search terms representative of the category.  With respect to reasons for use, there are three general reasons why the categories were selected. First, we desired a set of general indicators for social processes involved in revolution, in particular indicators for varying levels of unrest (protest, violence, and war) and indicators for processes of change (adaptation).  In addition, we included a large number of terrorist organizations.  The existence of terrorist organizations in a nation-state is known to be a general indicator of instability, as highly unstable countries are potential places where terrorist organizations can gain a foothold.  

Second, we included several concepts related to \citeapos{goldstone_cross-class_2011} theoretical model of cross-class revolution in order to better understand the extent to which his predictions are reflected in the language of social media and news media.  \citeapos{goldstone_cross-class_2011} recent assessment of the drivers of instability and revolutionary success during the Arab Spring suggests that a recurrent causal story played a role in the early revolutionary successes in Egypt, Tunisia and Libya.  Goldstone states that ``virtually all successful revolutions were forged by cross-class coalitions...pitting society as whole against the regime''. He then provides qualitative evidence that the revolutions in three countries were no different.  In Egypt, Tunisia and Libya, previously disparate social groups combined under a united ``cross-class coalition'' of protesters and revolutionaries, which made the revolutions significantly more likely to succeed.

Finally, we use a set of three ``noise categories'' to better understand the existence of spurious correlations that the methodology we utilize here may induce.  These noise categories were constructed by taking the top 25 most frequently used nouns and adjectives, as provided by Wikipedia\footnote{\url{http://en.wikipedia.org/wiki/Most_common_words_in_English}} and randomly splitting them into three categories.  Of these 50 terms, we removed five nouns (man, woman, child, life, government) and one verb (young) that were relevant to categories of interest, leaving us with 44 terms split between the three noise categories.

With respect to the number of terms used, it is clear that there is large variation across categories.  This imbalance is only to be expected- while certain categories, in particular the Terrorist Organization and Non-national identity ones, can contain a virtually limitless number of terms, many of the other categories (e.g. national identity) are restricted to a very limited number of possible terms.  However, there are two reasons why one should not expect these differences to affect our analyses. First, it is important to realize that the vast majority of terms in these two categories were found in only a few, if any, tweets and news articles. Second, as we show in the next sections, categories are analyzed independently and only compared based on rates of change.

\subsection{Statistical Approach}

The statistical model we utilize is drawn from the work of \cite{eisenstein_diffusion_2014}. Eisenstein and his colleagues were interested in understanding the extent to which new words diffused on Twitter across metro areas in the United States.  To capture the extent to which words in one metro area $r_1$ spread to a different area $r_2$, they formulated an autoregressive model that captured the extent to which the level of ``activation'' for the words in their dataset at time $t-1$ in metro area $r_1$ predicted the level in $r_2$ at time $t$.   As noted above, a na\"{i}ve use of count data, or proportions based on count data, would provide a biased analysis.  Briefly, this bias can be broken into two general, exogenous factors that influence fluctuations in both counts and proportions beyond the desired influence relationship.  First, as \cite{eisenstein_diffusion_2014} notes, sampling rates in the data may differ \emph{over time} due to fluctuations in the rate at which Twitter's API provided data over time.  These temporal patterns may also lead to random spikes in the data, spikes which may require smoothing.  Second, as our sample differentially focused on different countries (or in \cite{eisenstein_diffusion_2014}, because people tweeted more regularly from larger metro areas), there also exists and exogenous \emph{spatial} factor on count data that one does not actually wish to model\footnote{For more detail on these biases, we refer the reader to the original article}.

Our interest in the present work is not to understand diffusion, but rather to capture the extent to which different categories we are interested in were being discussed in different countries at different times.  Thus, large portions of the work performed by \cite{eisenstein_diffusion_2014} still apply, in particular the way in which they partial out temporal and spatial biases in their data. While the autoregressive portion of their model is not used here to estimate a diffusion network, we find that it still allows for a desirable smoothing of estimates over time that provides a more realistic picture of long-term change in discussion topics.  

\begin{table}[t]
	\centering
	\begin{tabularx} {\textwidth}{ |l | X| }
	\hline 
		{\bf Parameter} & {\bf Explanation} \\ \hline 
		$c_{w,r,t,m}$ & The $c$ount, or number, of users (newspaper articles) that mentioned category $w$ related to country $r$ at time $t$ in media $m$ \\ \hline 
		$s_{r,t,m}$ & The total number of users (newspaper articles) related to country $r$ at time $t$ in media $m$  \\ \hline 
		$\eta_{w,r,t,m}$ & The \emph{activation rate} of category $w$ related to country $r$ at time $t $ in media $m$ \\ \hline 
		$v_{w,t,m}$ & The activation rate of $w$ across all countries at time $t$ in media $m$  \\ \hline
		$\mu_{r,t,m}$ & The base activation rate of all users interested in country $r$ at time $t$ in media $m$  \\ \hline
		$A_{w,m}$ & The autoregressive component for each $w$ across all countries and times in media $m$  \\ \hline
		$\sigma_{w,r,m}$ & The standard deviation of $\eta$ draws for a particular $w$ and $r$ in media $m$  \\ \hline
	\end{tabularx}
	\caption{table}
	\label{tab:vars}
\end{table}

Table~\ref{tab:vars} presents an overview of the variables used in our description of the statistical model. Note that each model variable is differentiated by media via the subscript $m$. In all cases, results for the two media are estimated independently. Thus, to ease notation in this section, we will focus on Twitter, and consequently drop the $m$ subscript in our discussion.  The analog to a Twitter user is a newspaper article. All data is aggregated by month and by country.  From this aggregated data, we extract two primary values.  First, the variable $c_{w,r,t}$ gives the number of Twitter users (newspaper articles) who used category $w$ in at least one of their tweets related to the country $r$ during month $t$.  Second, $s_{r,t}$ represents the total number of users who sent one or more tweets about \emph{anything} in country $r$ during month $t$.  Thus, it is straightforward to represent $c_{w,r,t}$ as being distributed binomially, $c_{w,r,t} \sim \text{Bin}(s_{r,t},p)$.

It is obvious to see that the MLE for $p$, $\hat{p} = \frac{c_{w,r,t}}{s_{r,t}}$ in the naive model.  To address the biases above, \cite{eisenstein_diffusion_2014} introduce a logistic model for the $p$ parameter that allows an additive structure for parameters to estimate these biases independently, along with the ``true'' indicator of the popularity of category $w$ in country $r$ at time $t$. Equation~\ref{eq:c_eq} defines their basic model, where the logistic function $\text{Logistic}(x)$ is $\frac{1}{1 + \exp(x)}$.
\begin{equation}
	c_{w,r,t} \sim \mathrm{Binomial}(s_{r,t}, \mathrm{Logistic}(\eta_{w,r,t} + v_{w,t} + \mu_{r,t})) \label{eq:c_eq} 
\end{equation}
In Equation~\ref{eq:c_eq}, the three parameters in the logistic function affect the log-odds of an increase in $c_{w,r,t}$.  An increase in a parameter represents an increase in these log-odds, and thus the higher the value of each parameter, the more likely a user is to use the term.  \cite{eisenstein_diffusion_2014} uses the term ``activation'' to describe these increases, presumably in the context of the logistic model frequently used in cognitive activation theory \cite{}. We follow this terminology here.  In Equation~\ref{eq:c_eq}, the parameter $v_{w,t}$ is the overall activation of category $w$ at month $t$, and $\mu_{r,t}$ is the activation of the country $r$ at time $t$.  These terms control for temporal and spatial biases, respectively.  The parameter of interest to our analysis is thus $\eta_{w,r,t}$, which represented the unbiased activation for word $w$ at time $t$ in region $r$. 

As noted above, we also would like to smooth our estimates of word activations over time to better understand longer term trends in the data. As $\eta_{w,r,t}$ is our parameter of interest, the smoothing is enacted as an autoregressive model on $eta$, as described in Equation~\ref{eq:eta_eq}. Here, the parameter $A_{w,r}$ is the lagged influence variable, estimated for each category $w$ and each country $r$.
\begin{equation}
	\eta_{w,r,t} \sim \mathrm{N}( A_{w,r} \eta_{w,r',t-1}, \sigma^2_{w,r}) \label{eq:eta_eq}
\end{equation}

The full autoregression model can be specified as a Markov model with an \emph{observation model} (on $c$) and a \emph{dynamics}  model on $\eta$. Overall, the model we thus wish to estimate is:
\begin{equation}
	P(\eta,c|s; A, \sigma^2,\mu,v) = P(c | \eta, s; \mu, v) P(\eta; A)
\end{equation}

In order to do so, we adopt the same estimation process as was used by \cite{eisenstein_diffusion_2014}. We first consider the estimation of $v$ and $\mu$ for each word assuming $\eta$ is 0.  To do so, we utilize a stepwise procedure.  First, we obtain a simplified $\bar{v_w}$ as the inverse logistic function ($\log(\frac{x}{1-x})$) of the total proportion of users that utilized word $w$ across all time steps. Using this, we now would like to compute the maximum likelihood estimate of each $\mu_{r,t}$ using $\bar{v_w}$ as the value for each $v_{w,t}$ (and again, setting all $\eta = 0$).   Below we derive the MLE for a particular $\mu_r$ at a single time point $t$ (where the $t$ subscript is implicit), a step absent (presumably due to space) from the original article.  We first derive the form for the MLE:
\begin{align}
\begin{split}
  \hat{\mu_r} &= \mathrm{argmax}_{\mu} \prod_w P(c_w | s; \mu, v_w) \\
  &= \mathrm{argmax}_{\mu} \prod_w   ({}^{c_w}_{s}) \frac{1}{1+\exp(-(v_w+\mu))}^{c_w} (1- \frac{1}{ 1+\exp(-(v_w+\mu))})^{s-c_w} \\
  &= \mathrm{argmax}_{\mu} \sum_w \log({}^{c_w}_{s})  + \log(\frac{1}{1+ \exp(-(v_w+\mu))}^{c_w}) + \log( (1- \frac{1}{ 1+ \exp(-(v_w+\mu))})^{s-c_w}) \\
  &= \mathrm{argmax}_{\mu}  \sum_w \log({}^{c_w}_{s})  - c_w \log(1+ \exp(-(v_w+\mu)) + (s-c_w) \log (1- \frac{1}{ 1+ \exp(-(v_w+\mu))})
  \end{split}
\end{align}

As there is no closed-form solution to the MLE, we compute the derivative of the above expression and use it for a gradient descent approach to maximizing the function above.  The gradient can be derived as follows:
\begin{align}
\begin{split}
 \frac{\partial}{\partial \mu} \sum_w \log({}^{c_w}_{s})  - c_w \log(1+ \exp(-(v_w+\mu))) + (s-c_w) \log (1- \frac{1}{ 1+ \exp(-(v_w+\mu))}) \\
  =  \sum_w \frac{(c_w-s) \exp(v_w+\mu)}{ 1+ \exp(v_w+\mu)} +\frac{c_w}{1+ \exp(v_w+\mu)} 
  \end{split} 
\end{align}
An analogous derivation can be constructed for each $v$, and we thus exclude it from the present work. Now that we have our estimates for $\mu$ and $v$, we will estimate values for $\eta_{w,r,t}$ and $\sigma^2_{w,r}$. Because the observation model is non-Gaussian, the traditional Kalman Filter algorithm to infer parameter values cannot be used. Instead, we resort to Bayesian methods to perform approximate draws from the distribution of the $\eta$ s over time. We can then use maximum likelihood estimation to update the diagonal of $\tilde{A_w}$ and $\sigma^2_{w,r}$.  We then update expectations for $\tilde{A_w}$ and $\sigma^2_{w,r}$ and iterate again to generate a draw from an updated version of the distribution for $\eta_{w,r,t}$. 

This process results in a Monte-Carlo EM algorithm to estimate $A_{w}$, $\sigma^2_{w,r}$ and $\eta_{w,r,t}$.  In the E step, we get an expectation for $\eta$ using Forwards-Filtering Backwards Sampling (FFBS) \cite{} . \cite{eisenstein_diffusion_2014} do not specify an initial distribution for $\eta_0$, so we draw from $\mathrm{N}(\hat{eta_0}, 1)$, where $\hat{eta_0}$ is the MLE for $eta_0$, given in Equation~\ref{eq:mle_eta_0}, where $p=\frac{c}{s}$:
\begin{align} 
\begin{split}
	\hat{\eta_0} &= \text{argmax}_{\eta_0}  p(\eta_0 | \sigma^2, c_{t=0},s_{t=0}, \mu_{t=0}, v_{t=0}) \\
	&= \log(\frac{p}{1-p}) -v_{t=0} -  \mu_{t=0}  \label{eq:mle_eta_0}
	\end{split}
\end{align}

After obtaining a value for $\eta$ at time 0, we proceed with FFBS with the proposal distribution $Q(x)$ equal to the transition distribution. In contrast to \cite{eisenstein_diffusion_2014}, who use a simple particle filter in the forward-filtering step, we found our estimates were much more stable when we used an additional resampling step during the filtering process. Thus, for each time point, we construct weights for each sample $\eta_{w,r,t}^{(m)}$ as:
\begin{equation} 
\omega_{w,r,t}^{(m)} = \omega_{w,r,t-1}^{(m)} * P(c_{w,r,t} | \eta_{w,r,t}, s_{w,t}; \mu_{r,t}, v_{w,t}) \\
\end{equation}
We then resample from this distribution, producing a set of samples for $\eta_{w,r,t}$ from a discrete approximation of the true distribution. This process, when run on each $\eta$, completes the ``filtering'' step on the forward pass.  On the backwards pass, we draw our samples for $eta_{w,r,t}$ using these weights.  This completes the E step, or in other words, provides our samples for $\eta_{w,r,t}$. The M step updates the MLEs for $A_w$ and $\sigma^2_{w,r}$. The update for $\tilde{A_w}$ is simply determined via least-squares estimation. The MLE for $\sigma^2_{w,r}$ can be solved in closed form, $\frac{1}{T}\sum_t^T(\eta_{w,r,t} - \tilde{a_w,r}\eta_{w,r,t-1})$.

The code for this estimation process is available at (REMOVED FOR BLIND REVIEW).  Note that the primary differences between our work and \cite{eisenstein_diffusion_2014} are two-fold. First, \cite{eisenstein_diffusion_2014} are interested in constructing a diffusion network, and thus continue with a further estimation step to approximate a full transition matrix $A$ across all regions.  Second, while \cite{eisenstein_diffusion_2014} focus on specific terms, we focus on collections of terms. However, the generalization is trivial, as the sum of a set of independent Binomial random variables is still binomially distributed.


\subsection{Analytical methods}
	