\section{Data}

\begin{table*}
	\centering
	\begin{tabularx}{\textwidth}{| m{3.7cm} | m{2cm} |m{2cm}| m{3cm}|X  |}
		\hline
		{\bf Country name} & {\bf Level of protests} & {\bf Government overthrown?} & {\bf Government response if not overthrown?} & {\bf Current Situation}  \\ \hline
		Bahrain & High & No & Violent  & Ongoing protests   \\ \hline
		Egypt & High & Yes (2/11, 7/13) &  & Ongoing protests\\ \hline
		Iran & Low &  No & Police intervention & Idle \\ \hline
		Iraq & Low & No & Police intervention & Civl war    \\ \hline
		Jordan & Moderate &  No & Some reform  & Idle\\ \hline
		Kuwait &  Moderate & No & Some reform & Idle \\ \hline
		Lebanon & Moderate & No & Some reform & Idle  \\ \hline
		Libya & High & Yes (9/11) & & Civil war (involving ISIS)  \\ \hline
		Saudi Arabia & Low & No & Violent & Idle  \\ \hline
		Syria & High & No & Violent & Civil war \\ \hline
		Tunisia & High & Yes (1/11) &  & Idle  \\ \hline
		United Arab Emirates (UAE)   &  Low & No & Limited  & Idle  \\ \hline
		Yemen &  High & Yes (2/11) &	 & Ongoing protests and violence \\ \hline
	\end{tabularx}
	\caption{The thirteen countries studied in the present work, along with a (very) brief description of important events within the countries during the Arab Spring}
	\label{tab:countries}
\end{table*}


The present work focuses on discussions of human-curated topical categories in the large corpora of news and Twitter data. Data collection and analysis was focused on thirteen Arab countries, plus Iran. The countries are listed in Table I, along with a brief description of how the Arab Spring affected that particular state and what was happening in the country in May 2015 . Below we describe in greater detail the Twitter and news datasets. For more information on the Twitter data, we refer the reader to (REMOVED FOR BLIND REVIEW) . For information on the news data, we refer the reader to (REMOVED FOR BLIND REVIEW). Note that we use a subset of the Twitter data described in previous work, and an expanded news dataset collected as the data used in previous work.

\subsection{Twitter data}

The present work utilizes a corpora of approximately 70M tweets collected between March 2011 and December 2012. The Twitter data was collected from two sources. The first utilized a variety of approaches, including geospatial bounding boxes, keyword searches and searches for specific Twitter users, to capture material that data collectors deemed relevant to the Arab Spring. As the data was obtained from an outside source, an exact listing of terms and spatial regions from which this set of data was collected is unavailable to us. However, most of the collection is known to have focused on events relevant to Egypt, Libya, Syria, Tunisia and Yemen. The second source of data is obtained from a 10\% sample of all Twitter data made available to researchers at our university. From this sample, we extracted all tweets that were geo-tagged from the Arab World and Iran. Any geo-tagged tweet from a particular country is assumed to be relevant to that country. In addition, any tweet that mentioned the country’s name, in Arabic or in English, or a tweet that used the name of any of the country’s five major (most populous) cities in English or in Arabic, was assumed to be relevant to that country. Note that the same tweet may be considered relevant to multiple countries. 

\subsection{News data}
The present work utilizes a collection of approximately 700K newspaper articles collected during the same time period as the Twitter data. All newspaper articles were extracted from the LexisNexis archive of major, English-speaking news agencies. We collected data that LexisNexis’ proprietary indexing algorithms labelled as being relevant to one of the thirteen countries of interest. Text analysis was conducted only on the body of the text for each article.

\section{Methodology}

There are two major components to the methodology utilized in this paper. First we explain  the manner in which we narrowed down the themes, or categories, of interest and how we selected the terminology or proxy measures for each of the the categories. We then describe the methodology to determine the extent to which each category was assessed in different countries over time. 
\subsection{Categories}

As the data we use in this article only covers periods after the first revolts in both Tunisia and Egypt, tthe categories we considered in our analysis were chiefly focused on identifying relevant themes regarding the spread of protests and the extent to which revolutions ``succeeded''. After discussion among the co-authors, three of whom are subject matter experts (SMEs), we initially settled on twenty categories of interest. After delineating the subset of categories of interest, our next task was to identify specific terms\footnote{Note that a term may comprise multiple words, e.g. ``Michael Jordan'' might be a single term for the category ``sport''} that, if mentioned in a tweet or news article, suggested a discussion pertinent to that topic. This list of terms, was partly derived from dictionaries developed over several years of study on the region, and was further refined by the authors to finalize the set of relevant terms for each category. Finally, every tweet and news article in our dataset was searched for these terms. Due to the relatively data-hungry nature of the statistical model described in the following section, we chose to omit any category that was found in fewer than 5,000 news articles or 5,000 tweets.  This left us with a final set of eleven categories that we considered in our analysis.

\begin{table*}[t]
\begin{tabularx}{\textwidth}{| m{4.5cm} | m{4cm} | m{1.5cm} | X |}
	\hline 
	{\bf Category} & {\bf Reason for Use} & {\bf Num. Terms} & {\bf Representative Terms} \\ \hline
	  Terrorist Orgs  & Indicator of instability & 3619 & Al-qaeda, terrorism \\ \hline
	   Adaptation & Indicator of change & 130 & adjustments,  amendment \\ \hline
      Protest &  Indicator of unrest & 19 & protests, demonstrations \\ \hline
   Violence & Indicator of unrest & 39 & violence, violent \\ \hline
    War & Indicator of unrest & 46 &  war, wartime \\ \hline
    National identities (Nationality) & \cite{goldstone_cross-class_2011}& 245 & Algerian, Syrian \\ \hline  
   Youth & \cite{goldstone_cross-class_2011} &   10 & young person, youngster \\ \hline
   Non-national, ethnic identities (Ethnic groups) & \cite{goldstone_cross-class_2011} & 1489 & Shi'a, Sunni \\ \hline
   Stopwords 1 & Check for spuriousness & 14 & good, person, problem  \\ \hline
 Stopwords 2 & Check for spuriousness & 16 & good, person, problem \\ \hline
 Stopwords 3 & Check for spuriousness & 14 & good, person, problem\\ \hline
\end{tabularx}
	\caption{Categories Used}
	\label{tab:categories}
\end{table*}

Table~\ref{tab:categories} presents the set of categories used in the present work, along with a description of why they were chosen, the number of terms that map to this category and a set of search terms representative of the category. Initially we sought a set of general indicators for social processes involved in revolution, particularly indicators that demonstrated varying levels of unrest (protest, violence, and war) and indicators for processes of change (adaptation). In addition, whether a terrorist organization existed in the country was included as theme as the presence of terrorist groups tends to be an indicator of instability (and is highly correlated with violence and revolution). Highly unstable countries also tend to have ‘ungovernable spaces’ absent of government control that are potential areas in which terrorist organizations can gain a foothold. 

We also included several concepts inspired by the work of Goldstone and others \citep[e.g.][]{cottle_media_2011} suggesting that ``virtually all successful revolutions were forged by cross-class coalitions...pitting society as whole against the regime'' \citep[][pg. 457]{goldstone_cross-class_2011}. Finally, we used a set of three ``noise categories'' to asses the existence of spurious correlations that the methodology we utilize here may induce.  These noise categories were constructed by taking the top 25 most frequently used nouns and adjectives, as provided by Wikipedia\footnote{\url{http://en.wikipedia.org/wiki/Most_common_words_in_English}} and randomly splitting them into three categories.  Of these 50 terms, we removed five relevant nouns (man, woman, child, life, government) and one verb (young), leaving us with 44 terms split between the three noise categories.

With respect to the number of terms used, it is clear that there is significant variation across all of the categories. This imbalance is to be expected - while certain categories, e.g., Terrorist Organizations and Ethnic identities, can contain a limitless number of terms, many of the other categories (e.g. national identity) are restricted to a small number of possible terms. There are two reasons why one should not expect these differences to affect our analyses. First, it is important to realize that the vast majority of terms in these two categories were found in only a few, if any, tweets and news articles. Second, as we show in the next sections, categories were analyzed independently and only compared based on rates of change.

\subsection{Statistical Approach}

The statistical model we used is drawn from the work of \cite{eisenstein_diffusion_2014}. Eisenstein and his colleagues were interested in understanding the extent to which new words diffused on Twitter across metro areas in the United States.  To capture the extent to which words in one metro area $r_1$ spread to a different area $r_2$, they formulated an autoregressive model that captured the extent to which the level of ``activation'' for the words in their dataset at time $t-1$ in metro area $r_1$ predicted the level in $r_2$ at time $t$.   As noted above, a na\"{i}ve use of count data, or proportions based on count data, would provide a biased analysis.  Briefly, this bias can be broken into two general, exogenous factors that influence fluctuations in both counts and proportions beyond the desired influence relationship.  First, as \cite{eisenstein_diffusion_2014} notes, sampling rates in the data may differ \emph{over time} due to fluctuations in the rate at which Twitter's API provided data over time.  These temporal patterns may also lead to random spikes in the data, spikes which may require smoothing.  Second,  there also exists an exogenous \emph{spatial} factor on count data that one does not actually wish to model, particularly in that particular countries (metro areas) have much larger populations than others   \footnote{For more detail on these biases, we refer the reader to the original article}.

Our interest in the present work is not to understand diffusion, but rather to capture the extent to which different categories we are interested in were being discussed in different countries at different times.  Thus, large portions of the work performed by \cite{eisenstein_diffusion_2014} still apply, in particular the way in which they partial out temporal and spatial biases in their data. While the autoregressive portion of their model is not used here to estimate a diffusion network, we find that it still allows for a desirable smoothing of estimates over time. As we will discuss, it also makes it easy to remove temporal autocorrelation from the final estimates. 

\begin{table*}[ht]
	\centering
	\begin{tabularx} {\textwidth}{ |l | X| }
	\hline 
		{\bf Parameter} & {\bf Explanation} \\ \hline 
		$c_{w,r,t,m}$ & The $c$ount, or number, of users (newspaper articles) that mentioned category $w$ related to country $r$ at time $t$ in media $m$ \\ \hline 
		$s_{r,t,m}$ & The total number of users (newspaper articles) related to country $r$ at time $t$ in media $m$  \\ \hline 
		$\eta_{w,r,t,m}$ & The \emph{activation rate} of category $w$ related to country $r$ at time $t $ in media $m$ \\ \hline 
		$v_{w,t,m}$ & The activation rate of $w$ across all countries at time $t$ in media $m$  \\ \hline
		$\mu_{r,t,m}$ & The base activation rate of all users interested in country $r$ at time $t$ in media $m$  \\ \hline
		$A_{w,m}$ & The autoregressive component for each $w$ across all countries and times in media $m$  \\ \hline
		$\sigma_{w,r,m}$ & The standard deviation of $\eta$ draws for a particular $w$ and $r$ in media $m$  \\ \hline
	\end{tabularx}
	\caption{table}
	\label{tab:vars}
\end{table*}

Table~\ref{tab:vars} presents an overview of the variables used in our description of the statistical model. Note that each model variable is differentiated by media via the subscript $m$. In all cases, results for the two media are estimated independently. Thus, to ease notation in this section, we will focus on Twitter, and consequently drop the $m$ subscript in our discussion.  Note that the analog to a Twitter user in our discussion is a single newspaper article. All data is first aggregated by month and by country.  From this aggregated data, we extract two primary values.  First, the variable $c_{w,r,t}$ gives the number of Twitter users (newspaper articles) who used a term in category $w$ in at least one of their tweets related to the country $r$ during month $t$.  Second, $s_{r,t}$ represents the total number of users who sent one or more tweets about \emph{anything} in country $r$ during month $t$.  Thus, it is straightforward to represent $c_{w,r,t}$ as being distributed binomially, $c_{w,r,t} \sim \text{Bin}(s_{r,t},p)$.

It is obvious to see that the MLE for $p$, $\hat{p} = \frac{c_{w,r,t}}{s_{r,t}}$ in the naive model.  To address the biases above, \cite{eisenstein_diffusion_2014} introduce a logistic model for the $p$ parameter that allows an additive structure that incorporates  parameters to estimate these biases, along with the ``true'' indicator of the popularity of category $w$ in country $r$ at time $t$. Equation~\ref{eq:c_eq} defines their basic model, where the logistic function $\text{Logistic}(x)$ is $\frac{1}{1 + \exp(x)}$.
\begin{equation}
	c_{w,r,t} \sim \mathrm{Binomial}(s_{r,t}, \mathrm{Logistic}(\eta_{w,r,t} + v_{w,t} + \mu_{r,t})) \label{eq:c_eq} 
\end{equation}
In Equation~\ref{eq:c_eq}, the three parameters in the logistic function affect the log-odds of an increase in $c_{w,r,t}$.  An increase in a parameter represents an increase in these log-odds, and thus the higher the value of each parameter, the more likely a user is to use the term.  \cite{eisenstein_diffusion_2014} use the term ``activation'' to describe these increases, presumably in the context of the logistic model frequently used in cognitive activation theory \citep{anderson_act-r:_1997}. We follow this terminology here.  In Equation~\ref{eq:c_eq}, the parameter $v_{w,t}$ is the overall activation of category $w$ at month $t$, and $\mu_{r,t}$ is the activation of the country $r$ at time $t$.  These terms control for temporal and spatial biases, respectively.  The parameter of interest to our analysis is thus $\eta_{w,r,t}$, which represented the ``unbiased'' activation for word $w$ at time $t$ in region $r$. 

As noted above, we also would like to smooth our estimates of word activations over time to better understand longer term trends in the data. As $\eta_{w,r,t}$ is our parameter of interest, the smoothing is enacted as an autoregressive model on $\eta$, as described in Equation~\ref{eq:eta_eq}. Here, the parameter $A_{w,r}$ is the lagged influence variable, estimated for each category $w$ and each country $r$.
\begin{equation}
	\eta_{w,r,t} \sim \mathrm{N}( A_{w,r}*\eta_{w,r',t-1}, \sigma^2_{w,r}) \label{eq:eta_eq}
\end{equation}

The full autoregressive model can be specified as a Markov model with an \emph{observation model} (on $c$) and a \emph{dynamics}  model on $\eta$. Overall, the model we thus wish to estimate is:
\begin{equation}
	P(\eta,c|s; A, \sigma^2,\mu,v) = P(c | \eta, s; \mu, v) P(\eta; A)
\end{equation}

In order to do so, we adopted the same estimation process as was used by \cite{eisenstein_diffusion_2014}. We first considered the estimation of $v_{w,t}$ and $\mu_{r,t}$ at each time $t$ assuming $\eta$ is 0.  To do so, we utilize a stepwise procedure.  We first obtain a simplified $\bar{v_w}$ as the inverse logistic function ($\log(\frac{x}{1-x})$) of the total proportion of users that utilized word $w$ across all time steps, $t$. Using this value, we now would like to compute the maximum likelihood estimate of each $\mu_{r,t}$ using $\bar{v_w}$ as the value for each $v_{w,t}$ (and again, setting all $\eta = 0$).   Below we derive the MLE for a particular $\mu$ at an implicit region $r$ at a single time point $t$ (where the $t$ and $r$ subscripts are thus implicit). This derivation is  absent (presumably due to space) from \cite{eisenstein_diffusion_2014}, and thus we find it useful to outline here.  We first derive the form for the MLE:
\begin{align}
\small
\begin{split}
  \hat{\mu} &= \mathrm{argmax}_{\mu} \prod_w P(c_w | s; \mu, v_w) \\
  &= \mathrm{argmax}_{\mu} \prod_w   ({}^{c_w}_{s}) \frac{1}{1+\exp(-(v_w+\mu_r))}^{c_w}\\ &\qquad \qquad \qquad (1- \frac{1}{ 1+\exp(-(v_w+\mu_r))})^{s-c_w} \\
  &= \mathrm{argmax}_{\mu} \sum_w \log({}^{c_w}_{s})  + \log(\frac{1}{1+ \exp(-(v_w+\mu))}^{c_w}) \\ &\qquad \qquad \qquad + \log( (1- \frac{1}{ 1+ \exp(-(v_w+\mu))})^{s-c_w}) \\
  &= \mathrm{argmax}_{\mu}  \sum_w \log({}^{c_w}_{s})  - c_w \log(1+ \exp(-(v_w+\mu)) \\ &\qquad \qquad \qquad + (s-c_w) \log (1- \frac{1}{ 1+ \exp(-(v_w+\mu))})
  \end{split}
\end{align}

As there is no straightforward closed-form solution to this expression, we compute the derivative of the above expression and use it for a gradient descent approach to maximizing the function above.  The gradient can be derived as follows:
\begin{align}
\begin{split}
&  \frac{\partial}{\partial \mu} \sum_w \log({}^{c_w}_{s})  - c_w \log(1+ \exp(-(v_w+\mu))) + \\ &\qquad \qquad \qquad (s-c_w) \log (1- \frac{1}{ 1+ \exp(-(v_w+\mu))}) \\
  &=  \sum_w \frac{(c_w-s) \exp(v_w+\mu)}{ 1+ \exp(v_w+\mu)} +\frac{c_w}{1+ \exp(v_w+\mu)} 
  \end{split} 
\end{align}
An analogous derivation can be constructed for each $v$, and thus we excluded it from the present work.  After solving for all $w_{r,t}$ and all $v_{w,t}$, we can estimate values for parameters in the autoregressive portion of the model, $\eta_{w,r,t}$, $A_{w}$ and $\sigma^2_{w,r}$. Because the observation model here is non-Gaussian, the traditional Kalman Filter algorithm cannot be used to learn model parameters. Instead, we resort to Bayesian methods to perform approximate draws from the distribution of the $\eta$s over time. We can then use maximum likelihood estimation to update values for $A_{w}$ and $\sigma^2_{w,r}$.  We then update expectations for all $A$ and $\sigma^2$ parameters and iterate again to generate a draw from an updated version of the distribution for $\eta_{w,r,t}$. 

As outlined, this process is a Monte-Carlo EM algorithm \citep[p. 439]{bishop_pattern_2006}  that can be used to estimate $A_{w}$, $\sigma^2_{w,r}$ and $\eta_{w,r,t}$.  In the {\bf E} step, we get an expectation for $\eta$ using the Forwards-Filtering Backwards Sampling (FFBS) algorithm \citep{godsill_monte_2004}. To do so, we need an initial distribution, which we can use to begin the forward-filtering step in the algorithm. \cite{eisenstein_diffusion_2014} do not specify an initial distribution for $\eta_0$, so we draw from $\mathrm{N}(\hat{eta_0}, 1)$, where $\hat{\eta_0}$ is the MLE for $eta_0$, given in Equation~\ref{eq:mle_eta_0}, where $x=\frac{c}{s}$:
\begin{align} 
\begin{split}
	\hat{\eta_0} &= \text{argmax}_{\eta_0}  p(\eta_0 | \sigma^2, c_{t=0},s_{t=0}, \mu_{t=0}, v_{t=0}) \\
	&= \log(\frac{x}{1-x}) -v_{t=0} -  \mu_{t=0}  \label{eq:mle_eta_0}
	\end{split}
\end{align}

After obtaining a value for $\eta_{t=0}$, we proceed with FFBS with the proposal distribution $Q(x)$ equal to the transition distribution. In contrast to \cite{eisenstein_diffusion_2014}, who use a simple particle filter in the forward-filtering step, we found our estimates were much more stable when we used an additional resampling step during the filtering process  \citep{arulampalam_tutorial_2002}. Thus, for each time point, we construct weights for each sample $\eta_{w,r,t}^{(m)}$ as:
\begin{equation} 
\omega_{w,r,t}^{(m)} = \omega_{w,r,t-1}^{(m)} * P(c_{w,r,t} | \eta_{w,r,t}, s_{w,t}; \mu_{r,t}, v_{w,t}) \label{eq:s}
\end{equation}
Next, we resampled from this distribution, producing a set of samples for $\eta_{w,r,t}$ from a discrete approximation of the true distribution. This process, when run on each $\eta_{w,r,t}$, completes the ``filtering'' step on the forward pass.  On the backwards pass, we draw our samples for $eta_{w,r,t}$ using weights defined by the likelihood of each sample given the sample's likelihood as defined by the observation model. This completes the E step, or in other words, provides our samples for $\eta_{w,r,t}$. The M step updates the MLEs for $A_{w}$ and $\sigma^2_{w,r}$. The update for $A_w$ is simply determined via least-squares estimation. The MLE for $\sigma^2_{w,r}$ can be solved in closed form, $\frac{1}{T}\sum_t^T(\eta_{w,r,t} - \tilde{a_w,r}\eta_{w,r,t-1})$.

The code for this estimation process is available at (REMOVED FOR BLIND REVIEW).  Note that the primary differences between our work and \cite{eisenstein_diffusion_2014} are two-fold. First, \cite{eisenstein_diffusion_2014} were interested in constructing a diffusion network, and thus continue with a further estimation step to approximate a full transition matrix $A$ across all regions.  Second, while \cite{eisenstein_diffusion_2014} focus on specific terms, we focused on collections of terms. However, the generalization is trivial, as the sum of a set of independent Binomial random variables is still binomially distributed.
